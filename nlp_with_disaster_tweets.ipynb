{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zucuLWICoFS"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "00bAVSyWLRgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Change directory to where your CSV files are located\n",
        "os.chdir('/content/drive/My Drive/colab/nlp-with-disaster-tweets')"
      ],
      "metadata": {
        "id": "db0qtYgvLZt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load your dataset (train.csv)\n",
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "# Tokenize the text using BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenized_texts = [tokenizer.encode(text, add_special_tokens=True) for text in df['text']]\n",
        "\n",
        "# Pad and truncate the sequences to a fixed length\n",
        "max_len = max(len(tokens) for tokens in tokenized_texts)\n",
        "padded_sequences = [tokens + [0] * (max_len - len(tokens)) for tokens in tokenized_texts]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "input_ids = torch.tensor(padded_sequences)\n",
        "labels = torch.tensor(df['target'].values)"
      ],
      "metadata": {
        "id": "dvBXz7AnC7y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs, test_inputs, train_labels, test_labels = train_test_split(input_ids, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "m1pgcfQ7Lr3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "train_data = TensorDataset(train_inputs, train_labels)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "ExbY2JqKLuxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, AdamW\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)"
      ],
      "metadata": {
        "id": "gc9LxovMLyuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1\n",
        "\n",
        "batch_idx = 0\n",
        "epoch_idx = 0\n",
        "for epoch in range(num_epochs):\n",
        "  batch_idx = 0\n",
        "    epoch_idx = epoch_idx + 1\n",
        "    print(\"Processing Epoch\", epoch_idx)\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        batch_idx = batch_idx + 1\n",
        "        print(\"Processing Batch\", batch_idx)\n",
        "        inputs, labels = batch\n",
        "        outputs = model(inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "So_i5CyrL2ek"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}